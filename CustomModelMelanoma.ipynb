{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63a5dd0-2210-4cbc-bdc6-95367bc0419b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mosaicml\n",
      "  Obtaining dependency information for mosaicml from https://files.pythonhosted.org/packages/f6/db/35f413cd624b931c7f624a9e4dce320787d90b7a47a4011ba8acb9af715f/mosaicml-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading mosaicml-0.16.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (6.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.62.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (4.65.0)\n",
      "Collecting torchmetrics<1.1,>=0.10.0 (from mosaicml)\n",
      "  Obtaining dependency information for torchmetrics<1.1,>=0.10.0 from https://files.pythonhosted.org/packages/67/90/9ac94af10cd1777859a92be1e8186325490654930e871f8bb219cc342868/torchmetrics-1.0.3-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.0.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting torch-optimizer<0.4,>=0.3.0 (from mosaicml)\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision<0.16,>=0.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (0.15.2)\n",
      "Requirement already satisfied: torch<2.1,>=1.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (2.31.0)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.21.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (1.24.4)\n",
      "Requirement already satisfied: psutil<6,>=5.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (5.9.5)\n",
      "Collecting coolname<3,>=1.1.0 (from mosaicml)\n",
      "  Downloading coolname-2.2.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (0.9.0)\n",
      "Collecting py-cpuinfo<10,>=8.0.0 (from mosaicml)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: packaging<23,>=21.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (21.3)\n",
      "Requirement already satisfied: importlib-metadata<7,>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml) (6.8.0)\n",
      "Collecting mosaicml-cli<0.6,>=0.4.12 (from mosaicml)\n",
      "  Obtaining dependency information for mosaicml-cli<0.6,>=0.4.12 from https://files.pythonhosted.org/packages/c8/20/802d8e1ba87713ff3f3bf2971b7282fbd5f7179ff8df430568d473a700ce/mosaicml_cli-0.5.7-py3-none-any.whl.metadata\n",
      "  Downloading mosaicml_cli-0.5.7-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7,>=5.0.0->mosaicml) (3.16.2)\n",
      "Collecting argcomplete>=2.0.0 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Obtaining dependency information for argcomplete>=2.0.0 from https://files.pythonhosted.org/packages/4f/ef/8b604222ba5e5190e25851aa3a5b754f2002361dc62a258a8e9f13e866f4/argcomplete-3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading argcomplete-3.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: arrow>=1.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml-cli<0.6,>=0.4.12->mosaicml) (1.2.3)\n",
      "Collecting backoff>=2.2.1 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting gql[websockets]>=3.4.0 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading gql-3.4.1-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prompt-toolkit>=3.0.29 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml-cli<0.6,>=0.4.12->mosaicml) (3.0.39)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml-cli<0.6,>=0.4.12->mosaicml) (4.23.4)\n",
      "Collecting questionary>=1.10.0 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Obtaining dependency information for questionary>=1.10.0 from https://files.pythonhosted.org/packages/ec/16/d640510d6485660ef6c775a8ab92e4fdec4eb700d8a3749b729797a07c42/questionary-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading questionary-2.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting rich>=12.6.0 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Obtaining dependency information for rich>=12.6.0 from https://files.pythonhosted.org/packages/8d/5f/21a93b2ec205f4b79853ff6e838e3c99064d5dbe85ec6b05967506f14af0/rich-13.5.2-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml-cli<0.6,>=0.4.12->mosaicml) (0.17.32)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mosaicml-cli<0.6,>=0.4.12->mosaicml) (4.7.1)\n",
      "Collecting validators>=0.20.0 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Obtaining dependency information for validators>=0.20.0 from https://files.pythonhosted.org/packages/8d/e7/c2d0758c58dbd463b4a0a650f8cf3af5c4cbb6fbc61a36ce077964286feb/validators-0.21.2-py3-none-any.whl.metadata\n",
      "  Downloading validators-0.21.2-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging<23,>=21.3.0->mosaicml) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (2023.5.7)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<2.1,>=1.13.1->mosaicml) (3.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<2.1,>=1.13.1->mosaicml) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<2.1,>=1.13.1->mosaicml) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<2.1,>=1.13.1->mosaicml) (3.1.2)\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch-optimizer<0.4,>=0.3.0->mosaicml)\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting lightning-utilities>=0.7.0 (from torchmetrics<1.1,>=0.10.0->mosaicml)\n",
      "  Obtaining dependency information for lightning-utilities>=0.7.0 from https://files.pythonhosted.org/packages/46/ee/8641eeb6a062f383b7d6875604e1f3f83bd2c93a0b4dbcabd3150b32de6e/lightning_utilities-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision<0.16,>=0.13.1->mosaicml) (10.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from arrow>=1.2.2->mosaicml-cli<0.6,>=0.4.12->mosaicml) (2.8.2)\n",
      "Collecting graphql-core<3.3,>=3.2 (from gql[websockets]>=3.4.0->mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.6 (from gql[websockets]>=3.4.0->mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets<11,>=10 (from gql[websockets]>=3.4.0->mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompt-toolkit>=3.0.29->mosaicml-cli<0.6,>=0.4.12->mosaicml) (0.2.6)\n",
      "Collecting prompt-toolkit>=3.0.29 (from mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0 (from rich>=12.6.0->mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=12.6.0->mosaicml-cli<0.6,>=0.4.12->mosaicml) (2.15.1)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ruamel.yaml>=0.17.21->mosaicml-cli<0.6,>=0.4.12->mosaicml) (0.2.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch<2.1,>=1.13.1->mosaicml) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch<2.1,>=1.13.1->mosaicml) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow>=1.2.2->mosaicml-cli<0.6,>=0.4.12->mosaicml) (1.16.0)\n",
      "Collecting multidict>=4.0 (from yarl<2.0,>=1.6->gql[websockets]>=3.4.0->mosaicml-cli<0.6,>=0.4.12->mosaicml)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mosaicml-0.16.0-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.8/590.8 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mosaicml_cli-0.5.7-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading argcomplete-3.1.1-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading questionary-2.0.0-py3-none-any.whl (34 kB)\n",
      "Downloading rich-13.5.2-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.21.2-py3-none-any.whl (25 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py-cpuinfo, coolname, websockets, validators, prompt-toolkit, multidict, mdurl, graphql-core, backoff, argcomplete, yarl, questionary, markdown-it-py, lightning-utilities, torchmetrics, rich, pytorch-ranger, gql, torch-optimizer, mosaicml-cli, mosaicml\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.39\n",
      "    Uninstalling prompt-toolkit-3.0.39:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.39\n",
      "Successfully installed argcomplete-3.1.1 backoff-2.2.1 coolname-2.2.0 gql-3.4.1 graphql-core-3.2.3 lightning-utilities-0.9.0 markdown-it-py-3.0.0 mdurl-0.1.2 mosaicml-0.16.0 mosaicml-cli-0.5.7 multidict-6.0.4 prompt-toolkit-3.0.36 py-cpuinfo-9.0.0 pytorch-ranger-0.1.1 questionary-2.0.0 rich-13.5.2 torch-optimizer-0.3.0 torchmetrics-1.0.3 validators-0.21.2 websockets-10.4 yarl-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/5a/f2/5c2f878c62c8b79c629b11b33516bb55054d7677eba6f56f3a20296b56bd/tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/28/fa/c38a010d3fffcac07ef121abb34eb2c3db0876df74267ce5bde13c3a6ed7/grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/0d/7a/e55589e4093cca1934db5e99644c1c2424a9b3aac104b7f6176605a5eeb7/h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow)\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/6a/48/3cdab86db01701ea043de445f3af1c3e539835781e57d803d2a87f256475/tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.14)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, typing-extensions, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1-modules, oauthlib, numpy, markdown, keras, grpcio, gast, cachetools, astunparse, absl-py, requests-oauthlib, opt-einsum, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "triton 2.0.0 requires cmake, which is not installed.\n",
      "triton 2.0.0 requires lit, which is not installed.\n",
      "pydantic 2.0.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.3.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 grpcio-1.57.0 h5py-3.9.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 typing-extensions-4.5.0 wrapt-1.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting imbalanced-learn\n",
      "  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchcontrib\n",
      "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: torchcontrib\n",
      "  Building wheel for torchcontrib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7516 sha256=0e2b336a31441ddb94fc04b9331f2265a9a68a4ff39026f1c411249799ab2c7a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4e/e6/4d/ca9241b0fd2036630ed1674493738e27be708978362dac7cf4\n",
      "Successfully built torchcontrib\n",
      "Installing collected packages: torchcontrib\n",
      "Successfully installed torchcontrib-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mosaicml\n",
    "%pip install tensorflow\n",
    "%pip install imbalanced-learn\n",
    "%pip install torchcontrib\n",
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53eafc8b-ac8c-4c35-b100-cb0924dad4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib  import Path\n",
    "import pathlib\n",
    "dataset_path='./data/ham10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "907f9c05-4282-42f7-ad60-7e825dc7794c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from composer.models import ComposerModel\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import time\n",
    "import torchmetrics\n",
    "\n",
    "import torch.optim as optim\n",
    "from composer import Trainer\n",
    "import composer\n",
    "from composer.algorithms import BlurPool\n",
    "from composer.loggers import InMemoryLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torchvision.models.inception import InceptionOutputs\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchsummary import summary\n",
    "from composer.core import Evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bc7fb6-7cae-4c4d-afe7-f96f717f4d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir='./data/ham10000'\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "val_dir = os.path.join(base_dir, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25381fd5-2915-43d7-8406-1a0ed4177ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data=pd.read_csv(base_dir+'/HAM10000_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94120746-212b-4781-accb-4dae424bc526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0000004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0000007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0000008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>HAM_0000132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>HAM_0000133</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>HAM_0000134</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>HAM_0000137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>HAM_0000139</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lesion_id  image_id  dx  dx_type  age  sex  localization  dataset\n",
       "0   HAM_0000001         1   1        1    1    1             1        1\n",
       "1   HAM_0000003         1   1        1    1    1             1        1\n",
       "2   HAM_0000004         1   1        1    1    1             1        1\n",
       "3   HAM_0000007         1   1        1    1    1             1        1\n",
       "4   HAM_0000008         1   1        1    1    1             1        1\n",
       "..          ...       ...  ..      ...  ...  ...           ...      ...\n",
       "95  HAM_0000132         1   1        1    1    1             1        1\n",
       "96  HAM_0000133         1   1        1    1    1             1        1\n",
       "97  HAM_0000134         1   1        1    1    1             1        1\n",
       "98  HAM_0000137         1   1        1    1    1             1        1\n",
       "99  HAM_0000139         1   1        1    1    1             1        1\n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df = df[df['image_id'] == 1]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e43025-50e1-43ca-a8e3-1226a8492e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "base_dir = \"./data/ham10000\"  # Provide your directory path\n",
    "\n",
    "def transform_images(image_size, crop_size=None):\n",
    "    train_path = base_dir + '/Train/'\n",
    "    valid_path = base_dir + '/Test/'\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(image_size, interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    valid_transforms = transforms.Compose([\n",
    "        transforms.Resize(image_size, interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=train_path, transform=train_transforms)\n",
    "    valid_dataset = datasets.ImageFolder(root=valid_path, transform=valid_transforms)\n",
    "\n",
    "    train_batch_size = 10\n",
    "    val_batch_size = 10\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906ad276-7deb-4d65-a59a-638762ec8d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader, eval_dataloader, test_loader =transform_images((300,400),(300,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d800ece0-d006-480f-b549-3c8fb2fa01d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b782d7f7-f2b2-467b-adbd-51b538e3be80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepComposerCNN(ComposerModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DeepComposerCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc_dim = self.get_flattened_size()  # Adjusted based on the architecture\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_dim, 512)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 7)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x, _ = inputs \n",
    "        x = swish(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = swish(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = swish(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = swish(self.fc_bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = swish(self.fc_bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, outputs, inputs):\n",
    "        _, targets = inputs # Unpack the tuple\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    def get_flattened_size(self):\n",
    "        x = torch.randn(1, 3, 300, 400)  # Dummy input\n",
    "        x = self.pool1(swish(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(swish(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(swish(self.bn3(self.conv3(x))))\n",
    "        return x.view(-1).shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2eb440-8049-4f71-91c5-22ed19810d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242990087, 242990087)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = DeepComposerCNN()\n",
    "\n",
    "# Calculate number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params, trainable_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450998e5-af8d-4d27-a3f5-7bae07ba6f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_layers = sum(1 for _ in model.modules())\n",
    "total_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2664caed-a791-4fa2-bef5-ea524d15121a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 300, 400]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 300, 400]             128\n",
      "         MaxPool2d-3         [-1, 64, 150, 200]               0\n",
      "            Conv2d-4        [-1, 128, 150, 200]          73,856\n",
      "       BatchNorm2d-5        [-1, 128, 150, 200]             256\n",
      "         MaxPool2d-6         [-1, 128, 75, 100]               0\n",
      "            Conv2d-7         [-1, 256, 75, 100]         295,168\n",
      "       BatchNorm2d-8         [-1, 256, 75, 100]             512\n",
      "         MaxPool2d-9          [-1, 256, 37, 50]               0\n",
      "           Linear-10                  [-1, 512]     242,483,712\n",
      "      BatchNorm1d-11                  [-1, 512]           1,024\n",
      "          Dropout-12                  [-1, 512]               0\n",
      "           Linear-13                  [-1, 256]         131,328\n",
      "      BatchNorm1d-14                  [-1, 256]             512\n",
      "          Dropout-15                  [-1, 256]               0\n",
      "           Linear-16                    [-1, 7]           1,799\n",
      "================================================================\n",
      "Total params: 242,990,087\n",
      "Trainable params: 242,990,087\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 13.73\n",
      "Forward/backward pass size (MB): 230.68\n",
      "Params size (MB): 926.93\n",
      "Estimated Total Size (MB): 1171.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "model = model.cuda()\n",
    "\n",
    "# Use the summary function with the correct input size\n",
    "summary(model, input_size=(10,3, 300, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e712fab2-7bc3-4c28-bf7d-8d9bd82d1ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def requirementsForModelTraining(model):\n",
    "    blurpool = BlurPool(\n",
    "    replace_convs=True, # Blur before convs\n",
    "    replace_maxpools=True, # Blur before max-pools\n",
    "    blur_first=True # Blur before conv/max-pool)\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    algorithms=[blurpool]\n",
    "    return algorithms, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb1c880-bb98-447e-be60-f55544829503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import composer.functional as cf\n",
    "from composer.callbacks import ExportForInferenceCallback\n",
    "# change to 'torchscript' for exporting to torchscript format\n",
    "save_format = 'onnx'\n",
    "model_save_path = os.path.join(\"./models\", '202kPModel.onnx')\n",
    "export_callback = ExportForInferenceCallback(save_format=save_format, save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9182a92-3142-4649-bd7c-884e63dd09e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    dataloader = eval_dataloader,\n",
    "    label = \"eval\",\n",
    "    metric_names = ['MulticlassAccuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e5b521c-0818-482c-9c2a-0f847d8c9b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from composer.callbacks import EarlyStopper\n",
    "\n",
    "early_stopper = EarlyStopper(monitor=\"Accuracy\", dataloader_label=\"eval\",patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44e545b7-1a66-4e7b-bed6-34787397d4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "737666fa-6382-4a81-b7a8-79ef0e2ac8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from composer.trainer import Trainer\n",
    "from composer.loggers import InMemoryLogger\n",
    "\n",
    "logger = InMemoryLogger()\n",
    "# Assuming the CustomModelComposer class and other necessary code from previous steps are already executed\n",
    "\n",
    "# Create an instance of the model\n",
    "model=DeepComposerCNN()\n",
    "algorithms, optimizer,  = requirementsForModelTraining(model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizers=optimizer,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration='20ep',\n",
    "    loggers=logger,\n",
    "    algorithms=algorithms,\n",
    "    save_folder=\"checkpoints\",\n",
    "    save_filename=\"cmbpCB2008{epoch}.pt\",\n",
    "    save_interval=\"2ep\",\n",
    "    callbacks=[early_stopper, export_callback]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6aaf288c-2a58-4121-b3c0-a9e376fc3696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "blurpool/num_blurconv_layers: 0\n",
      "blurpool/num_blurpool_layers: 3\n",
      "enabled_algorithms/BlurPool: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 3525880457\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951def586de84e40849c43b19f05e4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/3889 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 14.75 GiB total capacity; 12.79 GiB already allocated; 888.81 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# <-- Your training loop in action!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/composer/trainer/trainer.py:1872\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, spin_dataloaders, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m ClosureGradScaler() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_closures() \u001b[38;5;28;01melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_batch_complete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1872\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/composer/trainer/trainer.py:2047\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_metrics({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mtoken\u001b[38;5;241m.\u001b[39mvalue})\n\u001b[1;32m   2045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_metrics({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/token_in_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mtoken_in_epoch\u001b[38;5;241m.\u001b[39mvalue})\n\u001b[0;32m-> 2047\u001b[0m total_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_grad_scaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_grad_scaling:\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/composer/trainer/trainer.py:2234\u001b[0m, in \u001b[0;36mTrainer._train_batch\u001b[0;34m(self, use_grad_scaling)\u001b[0m\n\u001b[1;32m   2231\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m loss_dict\u001b[38;5;241m=\u001b[39mtotal_loss_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_microbatches(\n\u001b[1;32m   2232\u001b[0m                 microbatches, loss_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_microbatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmicrobatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdeepspeed_enabled:\n\u001b[1;32m   2236\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m optimizer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moptimizers:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/composer/trainer/trainer.py:2334\u001b[0m, in \u001b[0;36mTrainer._train_microbatches\u001b[0;34m(self, microbatches, total_loss_dict, ddp_sync)\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m microbatch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(microbatches):\n\u001b[1;32m   2333\u001b[0m     is_final_microbatch \u001b[38;5;241m=\u001b[39m microbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(microbatches)\n\u001b[0;32m-> 2334\u001b[0m     microbatch_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_microbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_grad_scaling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_final_microbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m     \u001b[38;5;66;03m# Aggregate each loss in microbatch_loss_dict into total_loss_dict\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, microbatch_loss \u001b[38;5;129;01min\u001b[39;00m microbatch_loss_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/composer/trainer/trainer.py:2463\u001b[0m, in \u001b[0;36mTrainer._train_microbatch\u001b[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)\u001b[0m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2461\u001b[0m     \u001b[38;5;66;03m# Scale loss based on the number of samples in the microbatch to maintain gradient numerics\u001b[39;00m\n\u001b[1;32m   2462\u001b[0m     microbatch_loss\u001b[38;5;241m.\u001b[39mmul_(microbatch_num_samples \u001b[38;5;241m/\u001b[39m current_batch_size)\n\u001b[0;32m-> 2463\u001b[0m     \u001b[43mmicrobatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backwards_create_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mrun_event(Event\u001b[38;5;241m.\u001b[39mAFTER_BACKWARD)\n\u001b[1;32m   2467\u001b[0m \u001b[38;5;66;03m# Use microbatch outputs to update training metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 14.75 GiB total capacity; 12.79 GiB already allocated; 888.81 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit() # <-- Your training loop in action!\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f263ce0-d9a4-4009-bd2c-73658cffc5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "num_of_classes=7\n",
    "def evaluate_model(model):  \n",
    "    model.eval()\n",
    "    accuracy_metric = torchmetrics.classification.MulticlassAccuracy(num_classes=num_of_classes, average='micro')\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in eval_dataloader:\n",
    "            # Move data to the same device as the model (CPU or GPU)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Perform the forward pass and get the outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class by finding the index with the highest score\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Accumulate the true labels and predicted labels for each batch\n",
    "            true_labels.extend(targets.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "    # Convert the lists to numpy arrays\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the results\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on the evaluation dataset: {accuracy:.2f}%\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each class\n",
    "    TP = np.diag(conf_matrix)\n",
    "    FP = np.sum(conf_matrix, axis=0) - TP\n",
    "    FN = np.sum(conf_matrix, axis=1) - TP\n",
    "    TN = np.sum(conf_matrix) - (TP + FP + FN)\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce1d7d34-3842-433a-8485-35019586d8be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the evaluation dataset: 88.30%\n",
      "\n",
      "\n",
      "Precision: [0.66666667 0.67647059 0.63235294 0.75       0.46153846 0.9379085\n",
      " 0.63157895]\n",
      "Recall: [0.46666667 0.65714286 0.48863636 0.375      0.39130435 0.97508494\n",
      " 0.92307692]\n",
      "F1 Score: [0.54901961 0.66666667 0.55128205 0.5        0.42352941 0.95613548\n",
      " 0.75      ]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2aaf28c-7732-4399-8091-5a209c2938f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + self.residual(identity))\n",
    "\n",
    "class CustomCNN(ComposerModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.res1 = ResBlock(3, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.res2 = ResBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.res3 = ResBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv2 = nn.Upsample(size=(295, 195), mode='bilinear', align_corners=True)\n",
    "        self.res4 = ResBlock(384, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.res5 = ResBlock(128, 64)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Conv2d(64, 7, kernel_size=1)  # 7 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        if isinstance(x, tuple):\n",
    "            x, _ = x\n",
    "        x1 = self.res1(x)\n",
    "        x2 = self.pool1(x1)\n",
    "\n",
    "        x2 = self.res2(x2)\n",
    "        x3 = self.pool2(x2)\n",
    "\n",
    "        x3 = self.res3(x3)\n",
    "        x = self.pool3(x3)\n",
    "\n",
    "        # Decoder\n",
    "        # Use F.interpolate to dynamically adjust the size based on x2\n",
    "        x = F.interpolate(x, size=(x2.size(2), x2.size(3)), mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = torch.cat([x, x2], dim=1)  # Skip connection\n",
    "        x = self.res4(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, x1], dim=1)  # Skip connection\n",
    "\n",
    "        x = self.res5(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def loss(self, outputs, inputs):\n",
    "        _, targets = inputs # Unpack the tuple\n",
    "        return F.cross_entropy(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43330e99-e89b-432e-803a-dffe7121b06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader, eval_dataloader, test_loader =transform_images((450,300),(450,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d73ac61-f9ae-4cc3-971f-3ca24df4ec88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from composer.trainer import Trainer\n",
    "from composer.loggers import InMemoryLogger\n",
    "\n",
    "logger = InMemoryLogger()\n",
    "# Assuming the CustomModelComposer class and other necessary code from previous steps are already executed\n",
    "\n",
    "# Create an instance of the model\n",
    "customResBlockModel=CustomCNN()\n",
    "algorithms, optimizer,  = requirementsForModelTraining(customResBlockModel)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=customResBlockModel,\n",
    "    optimizers=optimizer,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration='5ep',\n",
    "    loggers=logger\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c974be8-4eba-429b-82e0-54bc964a0276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit() # <-- Your training loop in action!\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "826e31ad-a0cb-4c9d-9bfe-8dc99137bbf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided model must be a subclass of ComposerModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDenseNet121\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m tuner\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[0;32mIn[36], line 56\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, train_loader, val_loader, test_loader, architecture, hidden_layers, classes, epochs, batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m test_loader\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m InMemoryLogger()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/composer/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, train_dataloader, train_dataloader_label, train_subset_num_batches, spin_dataloaders, max_duration, algorithms, algorithm_passes, optimizers, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_interval, eval_subset_num_batches, callbacks, loggers, run_name, progress_bar, log_to_console, console_stream, console_log_interval, log_traces, auto_log_hparams, load_path, load_object_store, load_weights_only, load_strict_model_weights, load_progress_bar, load_ignore_keys, load_exclude_algorithms, save_folder, save_filename, save_latest_filename, save_overwrite, save_interval, save_weights_only, save_num_checkpoints_to_keep, autoresume, deepspeed_config, fsdp_config, fsdp_auto_wrap, device, precision, device_train_microbatch_size, seed, deterministic_mode, dist_timeout, ddp_sync_strategy, profiler, python_log_level, compile_config)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_hyperparameters({\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(dist\u001b[38;5;241m.\u001b[39mget_world_size() \u001b[38;5;241m/\u001b[39m dist\u001b[38;5;241m.\u001b[39mget_local_world_size()),\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms_per_node\u001b[39m\u001b[38;5;124m'\u001b[39m: dist\u001b[38;5;241m.\u001b[39mget_local_world_size(),\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_name\u001b[39m\u001b[38;5;124m'\u001b[39m: os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNODENAME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown because NODENAME environment variable not set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1137\u001b[0m })\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmodel, ComposerModel):\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProvided model must be a subclass of ComposerModel.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# After running Event.INIT, then set the \"optional\" elements of state that could be passed in on FIT instead of INIT\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# Metrics and Evaluators\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# change the model, which could change what metrics are computed\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtrain_metrics \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_metrics(is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mValueError\u001b[0m: Provided model must be a subclass of ComposerModel."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "from composer.trainer import Trainer\n",
    "from composer.loggers import InMemoryLogger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CustomModel(ComposerModel):\n",
    "    def __init__(self, base_arch, hidden_layers, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        if base_arch == 'DenseNet121':\n",
    "            self.base_arch = models.densenet121(pretrained=True).features\n",
    "        elif base_arch == 'DenseNet201':\n",
    "            self.base_arch = models.densenet201(pretrained=True).features\n",
    "        \n",
    "        prev_layer_size = 1024  # Output size of the base architecture\n",
    "        \n",
    "        layers = []\n",
    "        for layer_size in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_layer_size, layer_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(layer_size),\n",
    "                nn.Dropout(0.36)\n",
    "            ])\n",
    "            prev_layer_size = layer_size\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_layer_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base_arch(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten\n",
    "        \n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class Tuner(object):\n",
    "    def __init__(self, train_loader, eval_dataloader, test_loader, architecture, hidden_layers, classes, epochs, batch_size):\n",
    "        self.base_arch = architecture\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.classes = classes\n",
    "        self.EPOCHS = epochs\n",
    "        self.BATCH_SIZE = batch_size\n",
    "\n",
    "        self.model = CustomModel(self.base_arch, self.hidden_layers, self.classes)\n",
    "        self.train_loader = train_loader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.logger = InMemoryLogger()\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            optimizers=Adam(self.model.parameters(), lr=0.001),\n",
    "            train_dataloader=self.train_loader,\n",
    "            eval_dataloader=self.eval_dataloader,\n",
    "            max_duration=self.EPOCHS,\n",
    "            loggers=self.logger\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        self.trainer.fit()\n",
    "        self.plot_loss()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.logger.metric_history['train_loss'], label=\"train_loss\")\n",
    "        plt.plot(self.logger.metric_history['eval_loss'], label=\"val_loss\")\n",
    "        plt.plot(self.logger.metric_history['train_accuracy'], label=\"train_acc\")\n",
    "        plt.plot(self.logger.metric_history['eval_accuracy'], label=\"val_acc\")\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "tuner = Tuner(train_loader, eval_dataloader, test_loader, 'DenseNet121', [128, 64], 7, 10, 32)\n",
    "tuner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df986f9-bdb7-47c8-a470-e3a740fcd978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
