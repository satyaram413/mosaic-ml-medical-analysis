{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /opt/homebrew/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/homebrew/lib/python3.10/site-packages (from onnx) (4.5.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from onnx) (1.24.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/homebrew/lib/python3.10/site-packages (from onnx) (4.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: onnxruntime in /opt/homebrew/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime) (22.0)\n",
      "Requirement already satisfied: flatbuffers in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime) (4.23.4)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime) (1.24.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install onnx\n",
    "%pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from composer.models import ComposerModel\n",
    "import torchvision\n",
    "import os\n",
    "# Instantiate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model_save_path = os.path.join('./models/143kBPModel.onnx')\n",
    "onnx_model = onnx.load(model_save_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor: (1, 3, 300, 400)\n",
      "Expected input shape of the model: [10, 3, 300, 400]\n",
      "The predicted class is [5 5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Load the image\n",
    "image_path = \"./data/ham10000/Test/nv/ISIC_0024334.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the preprocessing steps\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((300,400), interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop((300,400)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # Add any other preprocessing steps if needed, like normalization\n",
    "])\n",
    "\n",
    "# Apply the transformations\n",
    "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Convert tensor to numpy\n",
    "input_numpy = input_tensor.numpy()\n",
    "input_numpy_batch = np.repeat(input_numpy, 10, axis=0)\n",
    "# Load the ONNX model and run inference\n",
    "ort_session = ort.InferenceSession(model_save_path)\n",
    "\n",
    "# Get the model's input name\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "# Print the shape of the input tensor\n",
    "print(f\"Shape of input tensor: {input_numpy.shape}\")\n",
    "\n",
    "# Print the expected input shape of the ONNX model\n",
    "input_shape = ort_session.get_inputs()[0].shape\n",
    "print(f\"Expected input shape of the model: {input_shape}\")\n",
    "\n",
    "# Run inference\n",
    "outputs = ort_session.run(None, {input_name: input_numpy_batch})\n",
    "\n",
    "# Print the predicted class\n",
    "print(f\"The predicted class is {np.argmax(outputs[0], axis=1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(ComposerModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet18()\n",
    "\n",
    "    def forward(self, batch):  # batch is the output of the dataloader\n",
    "        # Determine the format of the batch and handle it accordingly\n",
    "        if isinstance(batch, tuple):\n",
    "            # If the batch is in the format (inputs, targets)\n",
    "            inputs, targets = batch\n",
    "        else:\n",
    "            # If the batch is just the inputs\n",
    "            inputs = batch\n",
    "\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def loss(self, outputs, batch):\n",
    "        # pass batches and `forward` outputs to the loss\n",
    "        _, targets = batch\n",
    "        return F.cross_entropy(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "model.bn1.weight \t torch.Size([64])\n",
      "model.bn1.bias \t torch.Size([64])\n",
      "model.bn1.running_mean \t torch.Size([64])\n",
      "model.bn1.running_var \t torch.Size([64])\n",
      "model.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.layer1.0.bn1.weight \t torch.Size([64])\n",
      "model.layer1.0.bn1.bias \t torch.Size([64])\n",
      "model.layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "model.layer1.0.bn1.running_var \t torch.Size([64])\n",
      "model.layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.layer1.0.bn2.weight \t torch.Size([64])\n",
      "model.layer1.0.bn2.bias \t torch.Size([64])\n",
      "model.layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "model.layer1.0.bn2.running_var \t torch.Size([64])\n",
      "model.layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.layer1.1.bn1.weight \t torch.Size([64])\n",
      "model.layer1.1.bn1.bias \t torch.Size([64])\n",
      "model.layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "model.layer1.1.bn1.running_var \t torch.Size([64])\n",
      "model.layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.layer1.1.bn2.weight \t torch.Size([64])\n",
      "model.layer1.1.bn2.bias \t torch.Size([64])\n",
      "model.layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "model.layer1.1.bn2.running_var \t torch.Size([64])\n",
      "model.layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "model.layer2.0.bn1.weight \t torch.Size([128])\n",
      "model.layer2.0.bn1.bias \t torch.Size([128])\n",
      "model.layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "model.layer2.0.bn1.running_var \t torch.Size([128])\n",
      "model.layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.layer2.0.bn2.weight \t torch.Size([128])\n",
      "model.layer2.0.bn2.bias \t torch.Size([128])\n",
      "model.layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "model.layer2.0.bn2.running_var \t torch.Size([128])\n",
      "model.layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "model.layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "model.layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "model.layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "model.layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "model.layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.layer2.1.bn1.weight \t torch.Size([128])\n",
      "model.layer2.1.bn1.bias \t torch.Size([128])\n",
      "model.layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "model.layer2.1.bn1.running_var \t torch.Size([128])\n",
      "model.layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.layer2.1.bn2.weight \t torch.Size([128])\n",
      "model.layer2.1.bn2.bias \t torch.Size([128])\n",
      "model.layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "model.layer2.1.bn2.running_var \t torch.Size([128])\n",
      "model.layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "model.layer3.0.bn1.weight \t torch.Size([256])\n",
      "model.layer3.0.bn1.bias \t torch.Size([256])\n",
      "model.layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "model.layer3.0.bn1.running_var \t torch.Size([256])\n",
      "model.layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.layer3.0.bn2.weight \t torch.Size([256])\n",
      "model.layer3.0.bn2.bias \t torch.Size([256])\n",
      "model.layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "model.layer3.0.bn2.running_var \t torch.Size([256])\n",
      "model.layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "model.layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "model.layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "model.layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "model.layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "model.layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.layer3.1.bn1.weight \t torch.Size([256])\n",
      "model.layer3.1.bn1.bias \t torch.Size([256])\n",
      "model.layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "model.layer3.1.bn1.running_var \t torch.Size([256])\n",
      "model.layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.layer3.1.bn2.weight \t torch.Size([256])\n",
      "model.layer3.1.bn2.bias \t torch.Size([256])\n",
      "model.layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "model.layer3.1.bn2.running_var \t torch.Size([256])\n",
      "model.layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "model.layer4.0.bn1.weight \t torch.Size([512])\n",
      "model.layer4.0.bn1.bias \t torch.Size([512])\n",
      "model.layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "model.layer4.0.bn1.running_var \t torch.Size([512])\n",
      "model.layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.layer4.0.bn2.weight \t torch.Size([512])\n",
      "model.layer4.0.bn2.bias \t torch.Size([512])\n",
      "model.layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "model.layer4.0.bn2.running_var \t torch.Size([512])\n",
      "model.layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "model.layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "model.layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "model.layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "model.layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "model.layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.layer4.1.bn1.weight \t torch.Size([512])\n",
      "model.layer4.1.bn1.bias \t torch.Size([512])\n",
      "model.layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "model.layer4.1.bn1.running_var \t torch.Size([512])\n",
      "model.layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.layer4.1.bn2.weight \t torch.Size([512])\n",
      "model.layer4.1.bn2.bias \t torch.Size([512])\n",
      "model.layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "model.layer4.1.bn2.running_var \t torch.Size([512])\n",
      "model.layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.fc.weight \t torch.Size([1000, 512])\n",
      "model.fc.bias \t torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet18:\n\tMissing key(s) in state_dict: \"model.layer2.0.conv1.weight\", \"model.layer2.0.downsample.0.weight\", \"model.layer3.0.conv1.weight\", \"model.layer3.0.downsample.0.weight\", \"model.layer4.0.conv1.weight\", \"model.layer4.0.downsample.0.weight\". \n\tUnexpected key(s) in state_dict: \"model.maxpool.filt2d\", \"model.layer2.0.conv1.blur_filter\", \"model.layer2.0.conv1.conv.weight\", \"model.layer2.0.downsample.0.blur_filter\", \"model.layer2.0.downsample.0.conv.weight\", \"model.layer3.0.conv1.blur_filter\", \"model.layer3.0.conv1.conv.weight\", \"model.layer3.0.downsample.0.blur_filter\", \"model.layer3.0.downsample.0.conv.weight\", \"model.layer4.0.conv1.blur_filter\", \"model.layer4.0.conv1.conv.weight\", \"model.layer4.0.downsample.0.blur_filter\", \"model.layer4.0.downsample.0.conv.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./models/resnet18BP_20ep.pth\u001b[39;49m\u001b[39m'\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet18:\n\tMissing key(s) in state_dict: \"model.layer2.0.conv1.weight\", \"model.layer2.0.downsample.0.weight\", \"model.layer3.0.conv1.weight\", \"model.layer3.0.downsample.0.weight\", \"model.layer4.0.conv1.weight\", \"model.layer4.0.downsample.0.weight\". \n\tUnexpected key(s) in state_dict: \"model.maxpool.filt2d\", \"model.layer2.0.conv1.blur_filter\", \"model.layer2.0.conv1.conv.weight\", \"model.layer2.0.downsample.0.blur_filter\", \"model.layer2.0.downsample.0.conv.weight\", \"model.layer3.0.conv1.blur_filter\", \"model.layer3.0.conv1.conv.weight\", \"model.layer3.0.downsample.0.blur_filter\", \"model.layer3.0.downsample.0.conv.weight\", \"model.layer4.0.conv1.blur_filter\", \"model.layer4.0.conv1.conv.weight\", \"model.layer4.0.downsample.0.blur_filter\", \"model.layer4.0.downsample.0.conv.weight\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('./models/resnet18BP_20ep.pth', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
